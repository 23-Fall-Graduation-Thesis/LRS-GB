{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.0.1+cu117\n",
      "Torchvision Version:  0.15.2+cu117\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyprnt import prnt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" User Define function \"\"\"\n",
    "\n",
    "# from model.models import Conv4\n",
    "\n",
    "from model.pretrained_models import select_model\n",
    "from dataset_dir.datasets import datasetload\n",
    "\n",
    "from utils.util_functions import name_parser\n",
    "from utils.util_functions import get_alias\n",
    "\n",
    "from utils.visualize_tool import visualize_tensor\n",
    "from utils.visualize_tool import visualize_filters\n",
    "from utils.visualize_tool import visualize_feature_map\n",
    "from utils.visualize_tool import visualize_weight_distribution\n",
    "\n",
    "from utils.get_data import get_feature_from_dataset\n",
    "from utils.visualize_tool import visualize_feature_distribution\n",
    "from utils.util_functions import get_umap_embedding\n",
    "from utils.visualize_tool import visualize_class_activation_images\n",
    "from utils.visualize_grad import LayerCam\n",
    "from utils.util_functions import get_info\n",
    "\n",
    "from utils.get_data import get_performance_df\n",
    "from utils.visualize_tool import plot_comparison_each_dataset\n",
    "from utils.visualize_tool import plot_comparison_each_dataset_only_one\n",
    "from utils.visualize_tool import plot_comparison_each_dataset_only_two\n",
    "from utils.visualize_tool import visualize_gradXimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Directory Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IMPORTANT\n",
    "\n",
    "RECURSIVE\n",
    "- if you want visualization all weight in your directory, then RECURSIVE  = TRUE\n",
    "- else, you want visualization specific log in your directory, then RECURSIVE = FALSE\n",
    "\n",
    "SAVE\n",
    "- if you want save all visualization image, then SAVE = TRUE\n",
    "- else, you don't want save image, then SAVE = FALSE\n",
    "\n",
    "SHOW\n",
    "- if you want visualization result in this jupyter kernel, then SHOW = TRUE\n",
    "- else, you don't want show image in jupyter kernel, then SHOW = FALSE\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "RECURSIVE = True\n",
    "SAVE = True\n",
    "SHOW = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific model setting: 특정한 모델을 확인하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE :\n",
    "    conf = dict(\n",
    "        epoch=100,\n",
    "        lr=0.001,\n",
    "        stepsize=50,\n",
    "        gamma=0.1,\n",
    "        model=\"resnet18\",\n",
    "        dataset=\"cub\",\n",
    "        pretrain=False,\n",
    "        mode=\"cus\"\n",
    "    )\n",
    "    MODEL_DIR = \"./model/best_weight/\"\n",
    "    LOG_DIR = \"./results/best_log/\"\n",
    "\n",
    "    name = str(conf['model'])+\"_\"+str(conf['dataset'])+\"_\"+str(conf['mode'])\n",
    "    model_path = MODEL_DIR + name +\".pt\"\n",
    "    log_path = LOG_DIR +  name\n",
    "\n",
    "\n",
    "    print(model_path)\n",
    "    print(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entire model setting: directory에 저장되어있는 전체 모델을 확인하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "./model/best_weight/resnet50_cars_standard.pt\n",
      "6\n",
      "./results/best_log/resnet50_cub_standard\n"
     ]
    }
   ],
   "source": [
    "if RECURSIVE :\n",
    "    MODEL_DIR = \"./model/best_weight\"\n",
    "    LOG_DIR = \"./results/best_log\"\n",
    "\n",
    "    weight_files = [os.path.join(MODEL_DIR, file) for file in os.listdir(MODEL_DIR) if os.path.isfile(os.path.join(MODEL_DIR, file))]\n",
    "    log_files = [os.path.join(LOG_DIR, file) for file in os.listdir(LOG_DIR)]\n",
    "\n",
    "    print(len(weight_files))\n",
    "    print(weight_files[0])\n",
    "    print(len(log_files))\n",
    "    print(log_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Architecture\n",
    "특정 데이터셋, 모델의 구조를 확인하기 위한 코드. RECURSIVE하게 모든 weight파일에 대해서 동작하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE :\n",
    "    trainloader, validloader, testloader, num_class = datasetload(conf['dataset'])\n",
    "    MODEL = select_model(conf['model'], num_class)\n",
    "    model_state = torch.load(model_path)\n",
    "    MODEL.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE :\n",
    "    print(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE :\n",
    "    summary(MODEL, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용하고자하는 모델의 layer에 대한 정보를 기록합니다. (e.g., Alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "CLASS_NUM = dict(\n",
    "    cifar100=100,\n",
    "    cub=200,\n",
    "    cars=196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer1.0', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.conv3', 'layer1.0.bn3', 'layer1.0.relu', 'layer1.0.downsample', 'layer1.0.downsample.0', 'layer1.0.downsample.1', 'layer1.1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.conv3', 'layer1.1.bn3', 'layer1.1.relu', 'layer1.2', 'layer1.2.conv1', 'layer1.2.bn1', 'layer1.2.conv2', 'layer1.2.bn2', 'layer1.2.conv3', 'layer1.2.bn3', 'layer1.2.relu', 'layer2', 'layer2.0', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.conv3', 'layer2.0.bn3', 'layer2.0.relu', 'layer2.0.downsample', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.conv3', 'layer2.1.bn3', 'layer2.1.relu', 'layer2.2', 'layer2.2.conv1', 'layer2.2.bn1', 'layer2.2.conv2', 'layer2.2.bn2', 'layer2.2.conv3', 'layer2.2.bn3', 'layer2.2.relu', 'layer2.3', 'layer2.3.conv1', 'layer2.3.bn1', 'layer2.3.conv2', 'layer2.3.bn2', 'layer2.3.conv3', 'layer2.3.bn3', 'layer2.3.relu', 'layer3', 'layer3.0', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.conv3', 'layer3.0.bn3', 'layer3.0.relu', 'layer3.0.downsample', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.conv3', 'layer3.1.bn3', 'layer3.1.relu', 'layer3.2', 'layer3.2.conv1', 'layer3.2.bn1', 'layer3.2.conv2', 'layer3.2.bn2', 'layer3.2.conv3', 'layer3.2.bn3', 'layer3.2.relu', 'layer3.3', 'layer3.3.conv1', 'layer3.3.bn1', 'layer3.3.conv2', 'layer3.3.bn2', 'layer3.3.conv3', 'layer3.3.bn3', 'layer3.3.relu', 'layer3.4', 'layer3.4.conv1', 'layer3.4.bn1', 'layer3.4.conv2', 'layer3.4.bn2', 'layer3.4.conv3', 'layer3.4.bn3', 'layer3.4.relu', 'layer3.5', 'layer3.5.conv1', 'layer3.5.bn1', 'layer3.5.conv2', 'layer3.5.bn2', 'layer3.5.conv3', 'layer3.5.bn3', 'layer3.5.relu', 'layer4', 'layer4.0', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.conv3', 'layer4.0.bn3', 'layer4.0.relu', 'layer4.0.downsample', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.conv3', 'layer4.1.bn3', 'layer4.1.relu', 'layer4.2', 'layer4.2.conv1', 'layer4.2.bn1', 'layer4.2.conv2', 'layer4.2.bn2', 'layer4.2.conv3', 'layer4.2.bn3', 'layer4.2.relu', 'avgpool', 'fc']\n",
      "conv2\n"
     ]
    }
   ],
   "source": [
    "temp_conf = name_parser(weight_files[0])\n",
    "num_class = CLASS_NUM[temp_conf['dataset']]\n",
    "temp_model = select_model(temp_conf['model'], num_class)\n",
    "temp_layer_names = [name for name, _ in temp_model.named_modules()]\n",
    "print(temp_layer_names)\n",
    "\n",
    "# setting\n",
    "if temp_conf['model'] == 'resnet18':\n",
    "    layer_name_list = ['conv1', 'layer1.0.conv2', 'layer1.1.conv2', 'layer2.0.conv2', 'layer2.1.conv2', \\\n",
    "                'layer3.0.conv2', 'layer3.1.conv2', 'layer4.0.conv2', 'layer4.1.conv2']\n",
    "    model_info = dict(\n",
    "    conv1=0, conv2=10, conv3=11, conv4=20, conv5=21, conv6=30, conv7=31, conv8=40, conv9=41)   \n",
    "elif temp_conf['model'] == 'resnet50':\n",
    "    layer_name_list = ['conv1', 'layer1.0.conv2', 'layer1.1.conv2', 'layer1.2.conv2', 'layer2.0.conv2', 'layer2.1.conv2', \\\n",
    "                'layer2.2.conv2', 'layer2.3.conv2', 'layer3.0.conv2', 'layer3.1.conv2', 'layer3.2.conv2', 'layer3.3.conv2', \\\n",
    "                'layer3.4.conv2', 'layer3.5.conv2', 'layer4.0.conv2', 'layer4.1.conv2', 'layer4.2.conv2']\n",
    "    model_info = dict(\n",
    "    conv1=0, conv2=10, conv3=11, conv4=12, conv5=20, conv6=21, conv7=22, conv8=23, conv9=30, conv10=31, conv11=32, conv12=33, conv13=34, conv14=35, conv15=40, conv16=41, conv17=42)   \n",
    "else: # alexnet\n",
    "    layer_name_list = ['features.0', 'features.3', 'features.6', 'features.8', 'features.10']\n",
    "    model_info = dict(\n",
    "    conv1=0,\n",
    "    conv2=3,\n",
    "    conv3=6,\n",
    "    conv4=8,\n",
    "    conv5=10)   \n",
    "print(get_alias(layer_name_list[1], model_info, model=temp_conf['model']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Dataset Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE :\n",
    "    print(num_class)\n",
    "    print(len(trainloader.dataset))\n",
    "    print(len(validloader.dataset))\n",
    "    print(len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE :\n",
    "    img_idx = 0\n",
    "    image = trainloader.dataset[img_idx][0]\n",
    "    print(image.shape)\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    visualize_tensor(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Train/Val Best Accuracy save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_accuracy(path):\n",
    "    event_acc = EventAccumulator(path)\n",
    "    event_acc.Reload()\n",
    "\n",
    "    if 'scalars' in event_acc.Tags():\n",
    "        scalar_tags = event_acc.Tags()['scalars']\n",
    "        \n",
    "        for i, tag in enumerate(scalar_tags):\n",
    "            values = event_acc.Scalars(tag)\n",
    "            max_value = max(v.value for v in values)\n",
    "            min_value = min(v.value for v in values)\n",
    "            if tag == 'Acc/train':\n",
    "                best_train_acc = max_value\n",
    "            elif tag == 'Acc/val':\n",
    "                best_val_acc = max_value\n",
    "            elif tag == 'Loss/train':\n",
    "                best_train_loss = min_value\n",
    "            elif tag == 'Loss/val':\n",
    "                best_val_loss = min_value\n",
    "    \n",
    "    return best_train_acc, best_val_acc, best_train_loss, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭─────────────────┬──────────────────────────────╮\n",
      "│cub-standard     │╭──────────────┬─────────────╮│\n",
      "│                 ││best_train_acc│98.7695541381││\n",
      "│                 ││              │836          ││\n",
      "│                 ││best_val_acc  │65.3878250122││\n",
      "│                 ││              │0703         ││\n",
      "│                 ││best_train_...│0.00050658808││\n",
      "│                 ││              │32262337     ││\n",
      "│                 ││best_val_loss │0.01892288401││\n",
      "│                 ││              │722908       ││\n",
      "│                 │╰──────────────┴─────────────╯│\n",
      "│cifar100-standard│╭──────────────┬─────────────╮│\n",
      "│                 ││best_train_acc│99.9850006103││\n",
      "│                 ││              │5156         ││\n",
      "│                 ││best_val_acc  │81.3099975585││\n",
      "│                 ││              │9375         ││\n",
      "│                 ││best_train_...│2.58607451542││\n",
      "│                 ││              │0478e-05     ││\n",
      "│                 ││best_val_loss │0.00985303334││\n",
      "│                 ││              │8917961      ││\n",
      "│                 │╰──────────────┴─────────────╯│\n",
      "│cars-GBweva      │╭──────────────┬─────────────╮│\n",
      "│                 ││best_train_acc│99.9079055786││\n",
      "│                 ││              │1328         ││\n",
      "│                 ││best_val_acc  │79.7421722412││\n",
      "│                 ││              │1094         ││\n",
      "│                 ││best_train_...│3.77765427401││\n",
      "│                 ││              │755e-05      ││\n",
      "│                 ││best_val_loss │0.01488611660││\n",
      "│                 ││              │8977318      ││\n",
      "│                 │╰──────────────┴─────────────╯│\n",
      "│cub-GBweva       │╭──────────────┬─────────────╮│\n",
      "│                 ││best_train_acc│98.7695541381││\n",
      "│                 ││              │836          ││\n",
      "│                 ││best_val_acc  │69.8915786743││\n",
      "│                 ││              │164          ││\n",
      "│                 ││best_train_...│2.14188230529││\n",
      "│                 ││              │54394e-06    ││\n",
      "│                 ││best_val_loss │0.02220506034││\n",
      "│                 ││              │7914696      ││\n",
      "│                 │╰──────────────┴─────────────╯│\n",
      "│cifar100-GBweva  │╭──────────────┬─────────────╮│\n",
      "│                 ││best_train_acc│99.9625015258││\n",
      "│                 ││              │789          ││\n",
      "│                 ││best_val_acc  │82.4400024414││\n",
      "│                 ││              │0625         ││\n",
      "│                 ││best_train_...│3.04200657410││\n",
      "│                 ││              │54714e-05    ││\n",
      "│                 ││best_val_loss │0.01306830253││\n",
      "│                 ││              │4520626      ││\n",
      "│                 │╰──────────────┴─────────────╯│\n",
      "│cars-standard    │╭──────────────┬─────────────╮│\n",
      "│                 ││best_train_acc│99.9232559204││\n",
      "│                 ││              │1016         ││\n",
      "│                 ││best_val_acc  │69.9815826416││\n",
      "│                 ││              │0156         ││\n",
      "│                 ││best_train_...│0.00035905526││\n",
      "│                 ││              │601709425    ││\n",
      "│                 ││best_val_loss │0.02005607821││\n",
      "│                 ││              │047306       ││\n",
      "│                 │╰──────────────┴─────────────╯│\n",
      "╰─────────────────┴──────────────────────────────╯\n"
     ]
    }
   ],
   "source": [
    "\"\"\" RECURSIVE : Running This!!!\"\"\"\n",
    "BEST_VALUES = {}\n",
    "\n",
    "for log_path in log_files :\n",
    "    conf = name_parser(log_path)\n",
    "    best_train_acc, best_val_acc, best_train_loss, best_val_loss = get_best_accuracy(log_path)\n",
    "    alias = conf[\"dataset\"]+\"-\"+conf[\"mode\"]\n",
    "    BEST_VALUES[alias] = {'best_train_acc': best_train_acc, 'best_val_acc': best_val_acc, 'best_train_loss': best_train_loss, 'best_val_loss': best_val_loss}\n",
    "\n",
    "prnt(BEST_VALUES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Train Log Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_log(path, isSave=False, overlap=True):\n",
    "#     conf = name_parser(path)\n",
    "#     event_acc = EventAccumulator(path)\n",
    "#     event_acc.Reload()\n",
    "\n",
    "#     if 'scalars' in event_acc.Tags():\n",
    "#         scalar_tags = event_acc.Tags()['scalars']\n",
    "\n",
    "#         if not overlap :\n",
    "#             plt.figure(figsize=(13, 10))\n",
    "#             sns.set()\n",
    "\n",
    "#         for i, tag in enumerate(scalar_tags):\n",
    "#             values = event_acc.Scalars(tag)\n",
    "#             steps = [v.step for v in values]\n",
    "#             data = [v.value for v in values]\n",
    "            \n",
    "#             df = pd.DataFrame({'Step': steps, tag: data})\n",
    "\n",
    "#             plt.subplot(2, 2, i + 1)\n",
    "#             sns.lineplot(x='Step', y=tag, data=df, label=conf['dataset']+\"(\"+conf['freeze']+\")\" )\n",
    "#             plt.xlabel('Steps')\n",
    "#             plt.ylabel(tag)\n",
    "#             plt.title(f'{tag} over Time')\n",
    "            \n",
    "#         plt.tight_layout()\n",
    "#         if not overlap :\n",
    "#             plt.show()\n",
    "\n",
    "# if not RECURSIVE :\n",
    "#     draw_log(log_path, SAVE)\n",
    "\n",
    "# else :\n",
    "#     plt.figure(figsize=(13, 10))\n",
    "#     sns.set()\n",
    "#     for log_path in log_files :\n",
    "#         draw_log(log_path)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. cnn Filter visualization\n",
    "visualize_filters : 모델을 이루는 각각의 convolution 필터의 가중치를 시각화 합니다. (constraint: 1st channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE:\n",
    "    layer_name = \"features.0\"\n",
    "    visualize_filters(model_info, MODEL, layer_name, ncols=32, nchannel=5, showAll=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" RECURSIVE : Running This!!!\"\"\"\n",
    "if RECURSIVE:\n",
    "    for weight_path in tqdm(weight_files) :\n",
    "        conf = name_parser(weight_path)\n",
    "        num_class = CLASS_NUM[conf['dataset']]\n",
    "\n",
    "        MODEL = select_model(conf['model'], num_class)\n",
    "        model_state = torch.load(weight_path)\n",
    "        MODEL.load_state_dict(model_state)\n",
    "            \n",
    "        # for layer_name in layer_name_list :\n",
    "        #     visualize_filters(model_info, MODEL, layer_name, conf, save=SAVE, show=SHOW, ncols=32, nchannel=5, showAll=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Feature map Visualize\n",
    "visualize_feature_map : 특정 입력 이미지에 대해 모델의 각 레이어를 통과했을 때 feature_map을 시각화 합니다. (constraint: 1st feature map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE :\n",
    "    layer_name = 'conv2'\n",
    "    input_image = torch.unsqueeze(image, 0)\n",
    "\n",
    "    activation = {}\n",
    "    visualize_feature_map(activation, MODEL, input_image, layer_name, model_info[layer_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" RECURSIVE : Running This!!!\"\"\"\n",
    "\n",
    "# if RECURSIVE :\n",
    "#     img_idx = 0\n",
    "#     img_dict = dict(\n",
    "#         cifar100=None,\n",
    "#         cub=None,\n",
    "#         cars=None\n",
    "#     )\n",
    "#     for dataset in img_dict:\n",
    "#         trainloader, validloader, testloader, num_class = datasetload(dataset, batch_size=64)\n",
    "#         image = trainloader.dataset[img_idx][0]\n",
    "#         input_image = torch.unsqueeze(image, 0)\n",
    "#         img_dict[dataset] = input_image\n",
    "#     for weight_path in weight_files :\n",
    "#         conf = name_parser(weight_path)        \n",
    "#         input_image = img_dict[conf['dataset']]\n",
    "#         MODEL = select_model(conf['model'], CLASS_NUM[conf['dataset']])\n",
    "#         model_state = torch.load(weight_path)\n",
    "#         MODEL.load_state_dict(model_state)\n",
    "#         MODEL.eval()\n",
    "#         # for layer_name in layer_name_list:\n",
    "#         #     activation = {}\n",
    "#         #     visualize_feature_map(activation, MODEL, input_image, layer_name, conf, save=SAVE, show=SHOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. weight distribution (each layer)\n",
    "visualize_weight_distribution : 모델을 이루는 각각의 레이어의 가중치 분포를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE :\n",
    "    visualize_weight_distribution(MODEL, violin_sample=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" RECURSIVE : Running This!!!\"\"\"\n",
    "# if RECURSIVE :\n",
    "#     for weight_path in weight_files :\n",
    "#         conf = name_parser(weight_path)\n",
    "#         num_class = CLASS_NUM[conf['dataset']]\n",
    "        \n",
    "#         MODEL = select_model(conf['model'], num_class)\n",
    "#         model_state = torch.load(weight_path)\n",
    "#         MODEL.load_state_dict(model_state)\n",
    "        \n",
    "#         # visualize_weight_distribution(MODEL, conf, save=SAVE, show=SHOW, violin_sample=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Dataset Distribution\n",
    "visualize_feature_distribution: 모델이 판단하는 CLASS와 실제 정답 CLASS의 분포를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE : \n",
    "    features, labels, preds = get_feature_from_dataset(MODEL, 1000, testloader, layer_name, model_info[layer_name])\n",
    "    # print(features.shape)\n",
    "    # print(labels.shape)\n",
    "    # print(preds.shape)\n",
    "\n",
    "    embedding = get_umap_embedding(features)\n",
    "    #print(embedding.shape)\n",
    "    \n",
    "    visualize_feature_distribution(embedding, labels, preds, conf, layer_name, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" RECURSIVE : Running This!!! \"\"\" # LAST #TODO\n",
    "# if RECURSIVE :\n",
    "#     for weight_path in weight_files :\n",
    "#         conf = name_parser(weight_path)\n",
    "#         trainloader, validloader, testloader, num_class = datasetload(conf['dataset'], batch_size=64)\n",
    "#         MODEL = select_model(conf['model'], num_class)\n",
    "#         model_state = torch.load(weight_path)\n",
    "#         MODEL.load_state_dict(model_state)\n",
    "        \n",
    "        # for layer_name in layer_name_list: \n",
    "        #     features, labels, preds = get_feature_from_dataset(MODEL, 1000, testloader, layer_name)\n",
    "        #     embedding = get_umap_embedding(features)\n",
    "        #     visualize_feature_distribution(embedding, labels, preds, conf, layer_name, num_class=7, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Gradient\n",
    "##### 2.5.1 Layer-CAM\n",
    "LayerCAM [16] is a simple modification of Grad-CAM [3], which can generate reliable class activation maps from different layers.\n",
    "visualize_class_activation_images: 모델이 이미지를 분류할 떄, 어떤 부분에 집중하여 판단하는지를 함께 표현하여 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE:\n",
    "    data =  trainloader.dataset[img_idx]\n",
    "    origin_img, prep_img, target_class = get_info(data)\n",
    "    layer_number = 12  # [0, 12]\n",
    "\n",
    "    layer_cam = LayerCam(MODEL, layer_number)\n",
    "    cam = layer_cam.generate_cam(prep_img, target_class)\n",
    "    visualize_class_activation_images(origin_img, cam, conf, layer_number)\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    visualize_tensor(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" RECURSIVE : Running This!!! \"\"\" #TODO\n",
    "# if RECURSIVE :\n",
    "#     img_idx = 0\n",
    "#     img_dict = dict(\n",
    "#         cifar100=None,\n",
    "#         cub=None,\n",
    "#         cars=None\n",
    "#     )\n",
    "#     for dataset in img_dict:\n",
    "#         trainloader, validloader, testloader, num_class = datasetload(dataset, batch_size=64)\n",
    "#         data = trainloader.dataset[img_idx]\n",
    "#         img_dict[dataset] = data\n",
    "#     for weight_path in weight_files :\n",
    "#         conf = name_parser(weight_path)        \n",
    "#         data = img_dict[conf['dataset']]\n",
    "#         origin_img, prep_img, target_class = get_info(data)\n",
    "#         MODEL = select_model(conf['model'], CLASS_NUM[conf['dataset']])\n",
    "#         model_state = torch.load(weight_path)\n",
    "#         MODEL.load_state_dict(model_state)\n",
    "#         MODEL.eval()\n",
    "#         for idx, layer in enumerate(layer_name_list):\n",
    "#             layer_cam = LayerCam(MODEL, layer)\n",
    "#             cam = layer_cam.generate_cam(prep_img, target_class)\n",
    "#             visualize_class_activation_images(origin_img, cam, conf, idx, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.2 Grad Times Image\n",
    "Another technique that is proposed is simply multiplying the gradients with the image itself.\n",
    "visualize_gradXimage: 모델이 주어진 이미지를 볼 때, 계산된 Gradient와 원본이미지의 곱으로 나타낸 그림을 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE:\n",
    "    data =  trainloader.dataset[img_idx]\n",
    "    origin_img, prep_img, target_class = get_info(data)\n",
    "\n",
    "    visualize_gradXimage(prep_img, target_class, MODEL, conf, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Plot saved to ./results/figure/standard/resnet50/cars/GradXImage/vanila_grad.png\n",
      "Plot saved to ./results/figure/standard/resnet50/cub/GradXImage/vanila_grad.png\n",
      "Plot saved to ./results/figure/GBweva/resnet50/cars/GradXImage/vanila_grad.png\n",
      "Plot saved to ./results/figure/GBweva/resnet50/cub/GradXImage/vanila_grad.png\n",
      "Plot saved to ./results/figure/standard/resnet50/cifar100/GradXImage/vanila_grad.png\n",
      "Plot saved to ./results/figure/GBweva/resnet50/cifar100/GradXImage/vanila_grad.png\n"
     ]
    }
   ],
   "source": [
    "# \"\"\" RECURSIVE : Running This!!! \"\"\" #TODO\n",
    "# if RECURSIVE :\n",
    "#     img_idx = 0\n",
    "#     img_dict = dict(\n",
    "#         cifar100=None,\n",
    "#         cub=None,\n",
    "#         cars=None\n",
    "#     )\n",
    "#     for dataset in img_dict:\n",
    "#         trainloader, validloader, testloader, num_class = datasetload(dataset, batch_size=64)\n",
    "#         data = trainloader.dataset[img_idx]\n",
    "#         img_dict[dataset] = data\n",
    "#     for weight_path in weight_files :\n",
    "#         conf = name_parser(weight_path)        \n",
    "#         data = img_dict[conf['dataset']]\n",
    "#         origin_img, prep_img, target_class = get_info(data)\n",
    "\n",
    "#         MODEL = select_model(conf['model'], CLASS_NUM[conf['dataset']])\n",
    "#         model_state = torch.load(weight_path)\n",
    "#         MODEL.load_state_dict(model_state)\n",
    "#         MODEL.eval()\n",
    "\n",
    "#         visualize_gradXimage(prep_img, target_class, MODEL, conf, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparison\n",
    "[!attention] 아래의 그래프를 그리기 전에, 반드시 1.3.을 사용하여 accuracy결과를 저장해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset, df_freezing = get_performance_df(BEST_VALUES)\n",
    "\n",
    "# for group_key, df in df_dataset.items():\n",
    "#     print(f\"Group: {group_key}\")\n",
    "#     print(df)\n",
    "#     print()\n",
    "# for group_key, df in df_freezing.items():\n",
    "#     print(f\"Group: {group_key}\")\n",
    "#     print(df)\n",
    "#     print()\n",
    "\n",
    "# print(\"01100 freezing 설정을 사용하여 cifar100 데이터셋으로 파인튜닝한 모델의 값에 접근\")\n",
    "# group_df = df_dataset['cifar100']\n",
    "# print(group_df.loc['best_train_acc', '01100'])\n",
    "# group_df = df_freezing['01100']\n",
    "# print(group_df.loc['best_train_acc', 'cifar100'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Accuracy / Loss (general)\n",
    "plot_comparison_each_dataset: 각 데이터셋 마다 layer-freezing 변화에 따른 accuracy, loss를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE:\n",
    "    plot_comparison_each_dataset(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cifar10'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" RECURSIVE SAVE : Running This!!! \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RECURSIVE:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mplot_comparison_each_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSHOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LRS-GB/utils/visualize_tool.py:258\u001b[0m, in \u001b[0;36mplot_comparison_each_dataset\u001b[0;34m(df_dataset, show, save)\u001b[0m\n\u001b[1;32m    255\u001b[0m axes \u001b[38;5;241m=\u001b[39m axes\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m--> 258\u001b[0m     group_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(keys) :\n\u001b[1;32m    260\u001b[0m         row_values \u001b[38;5;241m=\u001b[39m group_df\u001b[38;5;241m.\u001b[39mloc[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cifar10'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" RECURSIVE SAVE : Running This!!! \"\"\"\n",
    "if RECURSIVE:\n",
    "    plot_comparison_each_dataset(df_dataset, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Accuracy / Loss (one-layer)\n",
    "plot_comparison_each_dataset_only_one: 각 데이터셋 마다 단 하나의 layer만을 선택하여 학습한 경우 acc / loss를 그립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE:\n",
    "    plot_comparison_each_dataset_only_one(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RECURSIVE SAVE : Running This!!! \"\"\"\n",
    "if RECURSIVE:\n",
    "    plot_comparison_each_dataset_only_one(df_dataset, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Accuracy / Loss (two-layer)\n",
    "plot_comparison_each_dataset_only_two: 각 데이터셋 마다 2개의 layer만을 선택하여 학습한 경우 acc / loss를 그립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RECURSIVE:\n",
    "    plot_comparison_each_dataset_only_two(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RECURSIVE SAVE : Running This!!! \"\"\"\n",
    "if RECURSIVE:\n",
    "    plot_comparison_each_dataset_only_two(df_dataset, show=SHOW, save=SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALIVE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
